{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1232HOkYeHsCb0COQ_pNWeFOrdf0m_PeU","timestamp":1736083346532},{"file_id":"1RSancWvkY0Ih9nbzAySNY8X5pvYSmttZ","timestamp":1732099879219}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","\n","**2024.11.20**\n","\n","#데이터 전처리 2 및 모델 학습\n","\n","\n","\n","\n","    1. 관광지명\n","    2. 소재지(도로명 주소)\n","    3. 공공편익시설정보, 휴양 및 문화시설정보\n","    4. 관광지소개\n","\n"],"metadata":{"id":"OVJwJ6wZGI0q"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4tV9uvHNottu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###1. 말뭉치 데이터 전처리"],"metadata":{"id":"nGd4M3U9JzkO"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","\n","# Google Drive에 저장한 json 파일 불러오기\n","import json\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/Colab Notebooks/전국관광지정보표준데이터.json'\n","\n","with open(file_path, 'r') as file:\n","    data = json.load(file)\n","\n","# 필요한 정보만 추출\n","processed_data = [\n","    {\n","        \"name\": item[\"관광지명\"],\n","        \"region\": item.get(\"소재지(도로명 주소)\"),\n","        \"category\": \" / \".join(filter(None, [item.get(\"공공편익시설정보\"), item.get(\"휴양 및 문화시설정보\")])) or \"정보 없음\"\n","        \"description\": item.get(\"관광지소개\")\n","    }\n","    for item in data[\"items\"]\n","]\n","\n","# 데이터 프레임으로 변환 (시각적 확인 및 처리 용이)\n","df = pd.DataFrame(processed_data)\n","\n","# 전처리 완료된 데이터 저장\n","file_path = \"/content/sample_data/processed_travel_data.json\"\n","\n","with open(file_path, \"w\", encoding='utf-8') as file:\n","    json.dump(df, file)\n","    print(\"데이터 전처리 JSON 파일이 성공적으로 저장되었습니다.\")\n","\n"],"metadata":{"id":"S9jOCmawFLpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###2. 모델 학습 데이터 준비"],"metadata":{"id":"FpPF_RGcJ7al"}},{"cell_type":"code","source":["# Prompt와 Response 생성\n","prompt_response_pairs = [\n","    {\n","        \"prompt\": f\"{row['region']}에서 갈만한 {row['category']} 장소 추천해줘.\",\n","        \"response\": f\"{row['name']} - {row['description']}\"\n","    }\n","    for _, row in df.iterrows()\n","]\n","\n","# 학습 데이터 저장\n","file_path = \"/content/sample_data/travel_chatbot_data.json\"\n","\n","with open(file_path, \"w\", encoding='utf-8') as file:\n","    json.dump(prompt_response_pairs, file)\n","    print(\"학습 데이터 JSON 파일이 성공적으로 저장되었습니다.\")\n","\n","# 샘플 데이터 확인\n","print(prompt_response_pairs[:2])\n"],"metadata":{"id":"V5CsMvJWJwVJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###3. 모델 학습 및 추론"],"metadata":{"id":"RDZtFAtNKnuB"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n","\n","# KoGPT 모델 및 토크나이저 로드\n","model_name = \"skt/kogpt2-base-v2\"\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# Prompt 예제\n","prompt = \"서울에서 갈만한 자연 장소 추천해줘.\"\n","\n","# 입력 토큰 생성\n","input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","\n","# KoGPT로 텍스트 생성\n","output = model.generate(\n","    input_ids,\n","    max_length=50,   # 생성할 텍스트 길이\n","    num_return_sequences=1,\n","    do_sample=True,  # 랜덤 샘플링 활성화\n","    top_k=50,        # 상위 K개의 토큰만 고려\n","    top_p=0.95       # 상위 P 확률의 토큰만 고려\n",")\n","\n","# 결과 디코딩\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(\"Generated Text:\", generated_text)\n"],"metadata":{"id":"GCDjUll1Krg7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###4. Fine-Tuning"],"metadata":{"id":"GoAToGC1Lpr3"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","# 데이터셋 로드\n","with open('travel_chatbot_data.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Hugging Face 데이터셋 변환\n","dataset = Dataset.from_dict({\n","    \"prompt\": [item[\"prompt\"] for item in data],\n","    \"response\": [item[\"response\"] for item in data]\n","})\n"],"metadata":{"id":"QMHzzwJyLtwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","# 학습 데이터셋 준비\n","def preprocess_function(examples):\n","    inputs = [f\"Q: {q}\\nA:\" for q in examples[\"prompt\"]]\n","    targets = examples[\"response\"]\n","    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=50)\n","    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=50)\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","processed_dataset = dataset.map(preprocess_function, batched=True)\n","\n","# TrainingArguments 설정\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=4,\n","    num_train_epochs=3,\n","    save_steps=500,\n","    save_total_limit=2,\n","    fp16=True\n",")\n","\n","# Trainer 생성\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=processed_dataset,\n","    tokenizer=tokenizer\n",")\n","\n","# Fine-Tuning 시작\n","trainer.train()\n"],"metadata":{"id":"F-2fuRokLyzM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###5. 추론"],"metadata":{"id":"YANhGd99LvjN"}},{"cell_type":"code","source":["# Fine-Tuned 모델 로드\n","model.save_pretrained('./fine_tuned_kogpt')\n","tokenizer.save_pretrained('./fine_tuned_kogpt')\n","\n","# Fine-Tuned 모델 로드 후 추론\n","fine_tuned_model = GPT2LMHeadModel.from_pretrained('./fine_tuned_kogpt')\n","fine_tuned_tokenizer = PreTrainedTokenizerFast.from_pretrained('./fine_tuned_kogpt')\n","\n","# Fine-Tuned 모델 사용\n","prompt = \"서울에서 갈만한 문화 장소 추천해줘.\"\n","input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors='pt')\n","output = fine_tuned_model.generate(input_ids, max_length=50, do_sample=True, top_k=50, top_p=0.95)\n","print(fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True))\n"],"metadata":{"id":"YNMPHUblL6m_"},"execution_count":null,"outputs":[]}]}